{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYu8aXQzonA6"
      },
      "source": [
        "# **Assignment 1 - Language Models**\n",
        "#### **Due: September 27 (Tuesday), 2022**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6heZKTDpBqd"
      },
      "source": [
        "## **Notes**\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Welcome to CSE 527A. The goal for the first assignment is to make sure you are familiar with all the tools you need to complete the programming assignments for the course. \n",
        "\n",
        "Each assignment contains two parts: a written and coding portion. The coding portion for each homework assignment will be delivered through a Colaboratory notebook such as this one. Please use as many as code and markdown cells to run and explain all the steps you took in order to answer each question.\n",
        "\n",
        "### **Comments/Documentation**\n",
        "\n",
        "Please follow PEP 8 style guidelines (https://peps.python.org/pep-0008/) for commenting your code. Furthermore, please remember to manually save your work once in a while. If you are connected to a hosted runtime that if for whatever reason it disconnects you will have to rerun all connected code cells.\n",
        "\n",
        "### **Getting Started**\n",
        "\n",
        "In order to compile code efficiently please pay attention to if you are using a hardware accelerator or not. If you are directly calling libraries like Tensorflow, Keras, or Pytorch, it is advised to switch to a GPU.\n",
        "\n",
        "To access a GPU, go to `Edit->Notebook settings` and in the `Hardware accelerator` dropdown choose `GPU`. \n",
        "As soon as you run a code cell, you will be connected to a cloud instance with a GPU.\n",
        "Try running the code cell below to check that a GPU is connected (select the cell then either click the play button at the top left or press `Ctrl+Enter` or `Shift+Enter`).\n",
        "\n",
        "The free version of Google Colab will provide the necessary hardware for this course. Please keep in mind the RAM and Disk Space that you are allocated and that you are not given an infinite active runtime.\n",
        "\n",
        "If your local machine has a GPU that you find outperforms the cloud GPU then you can follow the necessary documentation to use a GPU with your environment.\n",
        "\n",
        "### **Lost GPU/TPU Access on Colab**\n",
        "\n",
        "If you are not allocated a GPU or cannot connect to a GPU (limits are reached for Collab), Kaggle also provides free access to GPUs and TPUs. Please transfer your work to a Kaggle runtime instance by downloading your file on Colab as a '.ipynb' file and importing the file into Kaggle.\n",
        "\n",
        "### **Submission Instructions**\n",
        "\n",
        "We will use Gradescope for assignment submission. You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them. Please note if designated output is cleared, you will receive a 0.\n",
        "\n",
        "To download this notebook, go to `File->Download .ipynb`.  Please rename the file to match the name in our file list. \n",
        "\n",
        "When submitting your ipython notebooks, make sure everything runs correctly if the cells are executed in order starting from a fresh session.  Note that just because a cell runs in your current session doesn't mean it doesn't rely on code that you have already changed or deleted.  If the code doesn't take too long to run, we recommend re-running everything with `Runtime->Restart and run all...`.\n",
        "\n",
        "When you upload your submission to the Gradescope assignment, you should get immediate feedback that confirms your submission was processed correctly. Note that Gradesope will allow you to submit multiple times before the deadline, and we will use the latest submission for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKiZhHZxDynm"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR_t_TmDD2cq",
        "outputId": "ef3ff4d8-aed5-4bec-8345-d3ecb7bac868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # one option to load datasets\n",
        "from google.colab import files\n",
        "drive.mount('/content/gdrive')\n",
        "!nvidia-smi -L # check if using GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXOJeg2s-CE"
      },
      "source": [
        "## **Problem 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIYVOS5YUBFA"
      },
      "source": [
        "## **1.1**\n",
        "\n",
        "Write a program to compute unsmoothed unigrams, bigrams, and trigrams (you may not import nltk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOb6PN5UO3h",
        "outputId": "73569322-7c4d-4061-86ca-f13fa0f45335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigrams= {'asdf.g': 1, 'hjk': 1, 'hohoh': 1}\n",
            "bigrams= {('Valkyria', 'Chronicles'): 1, ('Chronicles', 'III'): 1, ('III', 'Senjō'): 1, ('Senjō', 'no'): 1, ('no', 'Valkyria'): 1, ('Valkyria', '3'): 1, ('3', 'Chronicles'): 1, ('Chronicles', 'Japanese'): 1, ('Japanese', '戦場のヴァルキュリア3'): 1, ('戦場のヴァルキュリア3', 'lit'): 1, ('lit', 'Valkyria'): 1, ('Valkyria', 'of'): 1, ('of', 'the'): 1}\n",
            "trigrams= {('asdf.g', 'hjk', 'hohoh'): 1}\n"
          ]
        }
      ],
      "source": [
        "## your code here\n",
        "\n",
        "\n",
        "def ngram(n,text): #generic N gram\n",
        "  text = text.split()\n",
        "  ngrams = {}\n",
        "  # generate ngram by sliding window\n",
        "  i,j = 0,n\n",
        "  while j <= len(text):\n",
        "    if n==1:\n",
        "      ngrams[text[i]] = ngrams.get(text[i],0)+1\n",
        "    else:\n",
        "      ngrams[tuple(text[i:j])] = ngrams.get(tuple(text[i:j]),0)+1\n",
        "    i+=1\n",
        "\n",
        "    j+=1\n",
        "  return ngrams\n",
        "\n",
        "def unigram(text):\n",
        "  return ngram(1,text)\n",
        "\n",
        "def bigram(text):\n",
        "  return ngram(2,text)\n",
        "\n",
        "def trigram(text):\n",
        "  return ngram(3,text)\n",
        "\n",
        "\n",
        "## test the above code\n",
        "print('unigrams=',unigram('asdf.g hjk hohoh'))\n",
        "#print('unigrams=',unigram('a'))\n",
        "print('bigrams=',bigram(('Valkyria Chronicles III Senjō no Valkyria 3   Chronicles  Japanese  戦場のヴァルキュリア3  lit  Valkyria of the')))\n",
        "#print('bigrams=',bigram(('a')))\n",
        "print('trigrams=',trigram(('asdf.g hjk hohoh')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1mvZAiUEAq"
      },
      "source": [
        "## **1.2**\n",
        "\n",
        "Train your model on the Wikitext-2-v1 training corpus (https://huggingface.co/datasets/wikitext). Explain the differences between your most common unigrams, bigrams, trigrams (pad beginning and end of your sentences and please remove puncutation and unknown tokens from corpus).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klAE9P8BPntp",
        "outputId": "83acf472-c434-47e6-c142-80054bfc8cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "## your code here\n",
        "\n",
        "#install datasets if necessary\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7c9a3bd90f3a4324b33788b849f31ea9",
            "94eeaf7ba1694ad7a84ff91b2b73653c",
            "4214c0c350f24fa98c92058de1c65615",
            "ac08384de96e4ca0b90625f3249c8ed4",
            "898131a3885945ed8a7c8de077ce51d8",
            "0d074659f8f3479e96d0bb902e676939",
            "58a7ad8f14a949b3945265c252a88ffb",
            "f40c0205c4904b0ca8b10188789bbe67",
            "bc1025da4dad4ce28ca67c0ec65a33c9",
            "ded8295a8d7a479a946fdddf55d41de3",
            "381f51309e5b47b58acc61fe81aab4a7"
          ]
        },
        "id": "gkheb8MSUPou",
        "outputId": "9a824def-bd53-4482-948a-a457c378c6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c9a3bd90f3a4324b33788b849f31ea9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-v1', split =['train','validation','test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ytoZ5mPiQAyr"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "\n",
        "def remove_unk_punctuation(data):\n",
        "  '''\n",
        "  remove <unk> and punctuations\n",
        "  '''\n",
        "\n",
        "  prepared_data = \"\"\n",
        "  for token in  data:\n",
        "    \n",
        "    current_string = token['text'].strip().replace(\"<unk>\", \"\").translate(token['text'].maketrans('', '', string.punctuation))\n",
        "    if current_string!=\"\":\n",
        "      # if not empty string, concate\n",
        "      prepared_data+=current_string\n",
        "\n",
        "  return prepared_data\n",
        "\n",
        "train = remove_unk_punctuation(dataset[0])\n",
        "validation = remove_unk_punctuation(dataset[1])\n",
        "test = remove_unk_punctuation(dataset[2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T1MkWTeVaWEW"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "def sorted_ngrams(n,text):\n",
        "  ngrams = ngram(n,text)\n",
        "  ngrams_sorted = dict(sorted(ngrams.items(), key=lambda item: item[1],reverse = True))\n",
        "  top_ngrams = []\n",
        "  for key in ngrams_sorted.keys():\n",
        "    top_ngrams.append(key)\n",
        "  return  top_ngrams\n",
        "\n",
        "frequency_sorted_unigram = sorted_ngrams(1,train)\n",
        "frequency_sorted_bigram = sorted_ngrams(2,train)\n",
        "frequency_sorted_trigram = sorted_ngrams(3,train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_6tjFzS0hH6",
        "outputId": "a230c8d5-652e-45a8-f408-e32d2ec01074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', 'and', 'in', 'to', 'a', 'was', 'The', 's', 'that']\n",
            "[('of', 'the'), ('in', 'the'), ('to', 'the'), ('and', 'the'), ('on', 'the'), ('for', 'the'), ('from', 'the'), ('at', 'the'), ('by', 'the'), ('as', 'a')]\n",
            "[('one', 'of', 'the'), ('the', 'United', 'States'), ('as', 'well', 'as'), ('part', 'of', 'the'), ('the', 'end', 'of'), ('end', 'of', 'the'), ('in', 'the', 'United'), ('a', 'number', 'of'), ('at', 'the', 'time'), ('known', 'as', 'the')]\n"
          ]
        }
      ],
      "source": [
        "print(frequency_sorted_unigram[:10])\n",
        "print(frequency_sorted_bigram[:10])\n",
        "print(frequency_sorted_trigram[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op5AfEeGUH5q"
      },
      "source": [
        "## **1.3**\n",
        "\n",
        "Calculate the perplexity for each n-gram (unigram, bigram, and trigram) on all splits (traning, validataion, test sets of Wikitext-2). And, without writing code, discuss what might happen to the perplexity if you continue to increase the number of words in your n-gram (4-gram, 5-gram, etc.)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m2XCUHbSUR2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74704e27-6325-4e1b-e7d6-60f9ab8801d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigrams probability= {'asdf.g': 0.25, 'hjk': 0.25, 'hohoh': 0.25, 'uuuu': 0.25}\n",
            "bigrams probability= {('asdf.g', 'hjk'): 0.3333333333333333, ('hjk', 'hohoh'): 0.3333333333333333, ('hohoh', 'uuuu'): 0.3333333333333333}\n"
          ]
        }
      ],
      "source": [
        "## your code here\n",
        "def ngram_probability_calculation(ngram_dict):\n",
        "  dict_values = ngram_dict.values()\n",
        "  total = sum(dict_values)\n",
        "  ngrams_probability = {}\n",
        "  for key in ngram_dict.keys():\n",
        "    ngrams_probability[key]=ngram_dict[key]/total\n",
        "  return ngrams_probability\n",
        "\n",
        "\n",
        "\n",
        "## test the above code\n",
        "print('unigrams probability=',ngram_probability_calculation(unigram('asdf.g hjk hohoh uuuu')))\n",
        "print('bigrams probability=',ngram_probability_calculation(bigram('asdf.g hjk hohoh uuuu')))\n",
        "\n",
        "#print('bigrams=',ngram_probability_calculation(bigram(('Valkyria Chronicles III Senjō no Valkyria 3   Chronicles  Japanese  戦場のヴァルキュリア3  lit  Valkyria of the'))))\n",
        "\n",
        "\n",
        "#print('trigrams=',ngram_probability_calculation(trigram(('asdf.g hjk hohoh'))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_probability for uni,bi,trigram\n",
        "unigram_train_probability = ngram_probability_calculation(ngram(1,train))\n",
        "bigram_train_probability = ngram_probability_calculation(ngram(2,train))\n",
        "trigram_train_probability = ngram_probability_calculation(ngram(3,train))\n",
        "\n",
        "unigram_train_count = ngram(1,train)\n",
        "bigram_train_count = ngram(2,train)\n",
        "trigram_train_count = ngram(3,train)\n",
        "\n",
        "unigram_test_count = ngram(1,test)\n",
        "bigram_test_count = ngram(2,test)\n",
        "trigram_test_count = ngram(3,test)\n",
        "\n",
        "unigram_validation_count = ngram(1,validation)\n",
        "bigram_validation_count = ngram(2,validation)\n",
        "trigram_validation_count = ngram(3,validation)"
      ],
      "metadata": {
        "id": "ubfDeiDfLBTo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check the probability\n",
        "'''\n",
        "print(sum(unigram_train_probability.values()))\n",
        "print(sum(bigram_train_probability.values()))\n",
        "print(sum(trigram_train_probability.values()))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z2zMsVkCMORU",
        "outputId": "6f494669-47c1-4cf6-d7f4-eb10e61ee1d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(sum(unigram_train_probability.values()))\\nprint(sum(bigram_train_probability.values()))\\nprint(sum(trigram_train_probability.values()))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "\n",
        "def text_probability(text,n,ngram_trained_model,n_minus_one_trained_model=None):\n",
        "  text = text.split()\n",
        "  ngrams_list = []\n",
        "\n",
        "  i,j = 0,n\n",
        "  while j <= len(text):\n",
        "    if n==1:\n",
        "      ngrams_list.append(text[i])\n",
        "    else:\n",
        "      ngrams_list.append(tuple(text[i:j]))\n",
        "    i+=1\n",
        "    j+=1\n",
        "  \n",
        "\n",
        "  log_probability = 0\n",
        "  probability = 1\n",
        "  if n==1:\n",
        "    #ngram_trained_model = unigram_train_count\n",
        "    total_unigram = sum(ngram_trained_model.values())\n",
        "    for token in ngrams_list:\n",
        "    #if token in trained_model:\n",
        "      log_probability+=math.log(ngram_trained_model[token]/total_unigram,2)\n",
        "      probability*=ngram_trained_model[token]/total_unigram\n",
        "\n",
        "\n",
        "  elif n==2:\n",
        "    #ngram_trained_model = bigram_train_count\n",
        "    #n_minus_one_trained_model = unigram_train_count\n",
        "    for token in ngrams_list:\n",
        "      n_minus_one_token = token[0]\n",
        "      log_probability+=math.log(ngram_trained_model[token]/n_minus_one_trained_model[n_minus_one_token],2)\n",
        "      probability*=ngram_trained_model[token]/n_minus_one_trained_model[n_minus_one_token]\n",
        "      \n",
        "\n",
        "\n",
        "  elif n==3:\n",
        "    #ngram_trained_model = trigram_train_count\n",
        "    #n_minus_one_trained_model = bigram_train_count\n",
        "    for token in ngrams_list:\n",
        "      n_minus_one_token = token[:n-1]\n",
        "      log_probability+=math.log(ngram_trained_model[token]/n_minus_one_trained_model[n_minus_one_token],2)\n",
        "      probability*=ngram_trained_model[token]/n_minus_one_trained_model[n_minus_one_token]\n",
        "  else:\n",
        "    print(\"not implemented\")\n",
        "    sys.exit(1)\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  #perplexity\n",
        "  perplexity = math.pow(2,-log_probability/len(ngrams_list))\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  return perplexity #, probability, math.pow(2,log_probability), log_probability\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## test the above code\n",
        "#print('unigrams=',text_probability('asdf.g hjk hohoh',1))\n",
        "#print('unigrams=',text_probability('asdf.g hjk hohoh',2))\n",
        "#print('unirams=',text_probability(('Valkyria Chronicles III Senjō no Valkyria 3   Chronicles  Japanese  戦場のヴァルキュリア3  lit  Valkyria of the'),1))\n",
        "#print('bigrams=',text_probability(('Valkyria Chronicles III Senjō no Valkyria 3   Chronicles  Japanese  戦場のヴァルキュリア3  lit  Valkyria of the'),2))\n",
        "\n",
        "#print(text_probability(train,1))\n",
        "print(text_probability(train,1,unigram_train_count))\n",
        "print(text_probability(train,2,bigram_train_count,unigram_train_count))\n",
        "print(text_probability(train,3,trigram_train_count,bigram_train_count))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VoJ9UctDHfv",
        "outputId": "1990ffe2-4d88-4d76-c859-652d9fe85648"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1941.083712678701\n",
            "97.34964861873617\n",
            "5.429987233533894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7GvYRzjUK9x"
      },
      "source": [
        "## **1.4**\n",
        "\n",
        "Enable Laplace smoothing and Add-K smoothing (0.1,0.05,0.01) to your code. Discuss the changes in perplexity values between n-grams as you try different smoothing methods/values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aVkncRe1USvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948d165e-e659-4017-e31d-39f1913e799d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1}\n",
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1}\n",
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1, ('asdf.g', 'gg'): 0, ('gg', 'hjk'): 0, ('hjk', 'fh'): 0, ('fh', 'hohoh'): 0, ('hohoh', 'gg'): 0, ('gg', 'asdfg'): 0}\n",
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1}\n",
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1, ('asdf.g', 'gg'): 0, ('gg', 'hjk'): 0, ('hjk', 'fh'): 0, ('fh', 'hohoh'): 0, ('hohoh', 'gg'): 0, ('gg', 'asdfg'): 0}\n",
            "{('asdf.g', 'hjk'): 1, ('hjk', 'hohoh'): 1, ('asdf.g', 'gg'): 0, ('gg', 'hjk'): 0, ('hjk', 'fh'): 0, ('fh', 'hohoh'): 0, ('hohoh', 'gg'): 0, ('gg', 'asdfg'): 0}\n",
            "{('asdf.g', 'hjk'): 2, ('hjk', 'hohoh'): 2, ('asdf.g', 'gg'): 1, ('gg', 'hjk'): 1, ('hjk', 'fh'): 1, ('fh', 'hohoh'): 1, ('hohoh', 'gg'): 1, ('gg', 'asdfg'): 1}\n",
            "{('asdf.g', 'hjk'): 3, ('hjk', 'hohoh'): 3, ('asdf.g', 'gg'): 2, ('gg', 'hjk'): 2, ('hjk', 'fh'): 2, ('fh', 'hohoh'): 2, ('hohoh', 'gg'): 2, ('gg', 'asdfg'): 2}\n"
          ]
        }
      ],
      "source": [
        "## your code here\n",
        "\n",
        "\n",
        "def prepare_for_smoothing(train_ngram_count,test_ngram_count,validation_ngram_count): \n",
        "  result=train_ngram_count.copy()\n",
        "  for token in test_ngram_count:\n",
        "    if token not in train_ngram_count:\n",
        "      result[token]=0\n",
        "  for token in validation_ngram_count:\n",
        "    if token not in train_ngram_count:\n",
        "      result[token]=0\n",
        "  return result\n",
        "\n",
        "def smoothing(train_count, k=0):\n",
        "  result=train_count.copy()\n",
        "  for token in train_count:\n",
        "    result[token]+=k\n",
        "  return result\n",
        "\n",
        "## test the above code\n",
        "#ngram(1,text)\n",
        "toy_unigram_train=ngram(2,'asdf.g hjk hohoh')\n",
        "toy_unigram_test=ngram(2,'asdf.g gg hjk fh hohoh')\n",
        "toy_unigram_valid=ngram(2,'asdf.g hjk hohoh gg asdfg')\n",
        "print(toy_unigram_train)\n",
        "toy_unigram_train_forsmoothing=prepare_for_smoothing(toy_unigram_train,toy_unigram_test,toy_unigram_valid)\n",
        "print(toy_unigram_train)\n",
        "print(toy_unigram_train_forsmoothing)\n",
        "\n",
        "toy_unigram_train_k_0=smoothing(toy_unigram_train_forsmoothing)\n",
        "toy_unigram_train_k_1=smoothing(toy_unigram_train_forsmoothing,1)\n",
        "toy_unigram_train_k_2=smoothing(toy_unigram_train_forsmoothing,2)\n",
        "print(toy_unigram_train)\n",
        "print(toy_unigram_train_forsmoothing)\n",
        "print(toy_unigram_train_k_0)\n",
        "print(toy_unigram_train_k_1)\n",
        "print(toy_unigram_train_k_2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unigram_train_count_smoothing= prepare_for_smoothing(unigram_train_count,unigram_test_count,unigram_validation_count)\n",
        "bigram_train_count_smoothing = prepare_for_smoothing(bigram_train_count,bigram_test_count,bigram_validation_count)\n",
        "trigram_train_count_smoothing = prepare_for_smoothing(trigram_train_count,trigram_test_count,trigram_validation_count)"
      ],
      "metadata": {
        "id": "4AoTKh3z9OhU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_train_k_0=smoothing(unigram_train_count_smoothing)\n",
        "bigram_train_k_0=smoothing(bigram_train_count_smoothing)\n",
        "trigram_train_k_0=smoothing(trigram_train_count_smoothing)\n",
        "\n",
        "#Laplace smoothing\n",
        "unigram_train_k_laplace=smoothing(unigram_train_count_smoothing,1)\n",
        "bigram_train_k_laplace=smoothing(bigram_train_count_smoothing,1)\n",
        "trigram_train_k_laplace=smoothing(trigram_train_count_smoothing,1)\n",
        "\n",
        "#k(0.1,0.05,0.01) smoothing \n",
        "unigram_train_k_1=smoothing(unigram_train_count_smoothing,0.1)\n",
        "bigram_train_k_1=smoothing(bigram_train_count_smoothing,0.1)\n",
        "trigram_train_k_1=smoothing(trigram_train_count_smoothing,0.1)\n",
        "\n",
        "unigram_train_k_05=smoothing(unigram_train_count_smoothing,0.05)\n",
        "bigram_train_k_05=smoothing(bigram_train_count_smoothing,0.05)\n",
        "trigram_train_k_05=smoothing(trigram_train_count_smoothing,0.05)\n",
        "\n",
        "unigram_train_k_01=smoothing(unigram_train_count_smoothing,0.01)\n",
        "bigram_train_k_01=smoothing(bigram_train_count_smoothing,0.01)\n",
        "trigram_train_k_01=smoothing(trigram_train_count_smoothing,0.01)"
      ],
      "metadata": {
        "id": "-2wYpXBJG6Zx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_probability(train,1,unigram_train_k_0))\n",
        "print(text_probability(train,2,bigram_train_k_0,unigram_train_k_0))\n",
        "print(text_probability(train,3,trigram_train_k_0,bigram_train_k_0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPLGYwFBAMVi",
        "outputId": "f7cecbbf-6d9b-4c9d-d338-094fcfcf49ab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1941.083712678701\n",
            "97.34964861873617\n",
            "5.429987233533894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"......lapalce smooting performance.......\")\n",
        "\n",
        "print('..train..')\n",
        "print(\"unigram perplexity=\",text_probability(train,1,unigram_train_k_laplace))\n",
        "print(\"bigram perplexity=\",text_probability(train,2,bigram_train_k_laplace,unigram_train_k_laplace))\n",
        "print(\"trigram perplexity=\",text_probability(train,3,trigram_train_k_laplace,bigram_train_k_laplace))\n",
        "print('..validation..')\n",
        "print(\"unigram perplexity=\",text_probability(validation,1,unigram_train_k_laplace))\n",
        "print(\"bigram perplexity=\",text_probability(validation,2,bigram_train_k_laplace,unigram_train_k_laplace))\n",
        "print(\"trigram perplexity=\",text_probability(validation,3,trigram_train_k_laplace,bigram_train_k_laplace))\n",
        "print('..test..')\n",
        "print(\"unigram perplexity=\",text_probability(test,1,unigram_train_k_laplace))\n",
        "print(\"bigram perplexity=\",text_probability(test,2,bigram_train_k_laplace,unigram_train_k_laplace))\n",
        "print(\"trigram perplexity=\",text_probability(test,3,trigram_train_k_laplace,bigram_train_k_laplace))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mT4gnjvIh3o",
        "outputId": "8b23719a-e753-4777-deca-1ea3f133bfc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......lapalce smooting performance.......\n",
            "..train..\n",
            "unigram perplexity= 1943.6194148230973\n",
            "bigram perplexity= 73.77607260148193\n",
            "trigram perplexity= 4.176797835405882\n",
            "..validation..\n",
            "unigram perplexity= 1730.0201050331605\n",
            "bigram perplexity= 110.30615567874555\n",
            "trigram perplexity= 5.682695471903138\n",
            "..test..\n",
            "unigram perplexity= 1688.5167095041265\n",
            "bigram perplexity= 111.17559751168041\n",
            "trigram perplexity= 5.792542173852982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"......k=0.1 smooting performance.......\")\n",
        "\n",
        "print('..train..')\n",
        "print(\"unigram perplexity=\",text_probability(train,1,unigram_train_k_1))\n",
        "print(\"bigram perplexity=\",text_probability(train,2,bigram_train_k_1,unigram_train_k_1))\n",
        "print(\"trigram perplexity=\",text_probability(train,3,trigram_train_k_1,bigram_train_k_1))\n",
        "print('..validation..')\n",
        "print(\"unigram perplexity=\",text_probability(validation,1,unigram_train_k_1))\n",
        "print(\"bigram perplexity=\",text_probability(validation,2,bigram_train_k_1,unigram_train_k_1))\n",
        "print(\"trigram perplexity=\",text_probability(validation,3,trigram_train_k_1,bigram_train_k_1))\n",
        "print('..test..')\n",
        "print(\"unigram perplexity=\",text_probability(test,1,unigram_train_k_1))\n",
        "print(\"bigram perplexity=\",text_probability(test,2,bigram_train_k_1,unigram_train_k_1))\n",
        "print(\"trigram perplexity=\",text_probability(test,3,trigram_train_k_1,bigram_train_k_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7i5OvcqJxJS",
        "outputId": "c7a69309-f672-45a0-9f83-ee732e747687"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......k=0.1 smooting performance.......\n",
            "..train..\n",
            "unigram perplexity= 1941.1284835418628\n",
            "bigram perplexity= 93.8621401417523\n",
            "trigram perplexity= 5.233655962524139\n",
            "..validation..\n",
            "unigram perplexity= 1726.269501963434\n",
            "bigram perplexity= 251.70034887595685\n",
            "trigram perplexity= 14.707950555613726\n",
            "..test..\n",
            "unigram perplexity= 1684.858102481261\n",
            "bigram perplexity= 251.96117101111764\n",
            "trigram perplexity= 15.096859354326547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"......k=0.05 smooting performance.......\")\n",
        "\n",
        "print('..train..')\n",
        "print(\"unigram perplexity=\",text_probability(train,1,unigram_train_k_05))\n",
        "print(\"bigram perplexity=\",text_probability(train,2,bigram_train_k_05,unigram_train_k_05))\n",
        "print(\"trigram perplexity=\",text_probability(train,3,trigram_train_k_05,bigram_train_k_05))\n",
        "print('..validation..')\n",
        "print(\"unigram perplexity=\",text_probability(validation,1,unigram_train_k_05))\n",
        "print(\"bigram perplexity=\",text_probability(validation,2,bigram_train_k_05,unigram_train_k_05))\n",
        "print(\"trigram perplexity=\",text_probability(validation,3,trigram_train_k_05,bigram_train_k_05))\n",
        "print('..test..')\n",
        "print(\"unigram perplexity=\",text_probability(test,1,unigram_train_k_05))\n",
        "print(\"bigram perplexity=\",text_probability(test,2,bigram_train_k_05,unigram_train_k_05))\n",
        "print(\"trigram perplexity=\",text_probability(test,3,trigram_train_k_05,bigram_train_k_05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6IaTbBl0Smb",
        "outputId": "b15ac9e3-3bc8-435b-81f9-ce61fbd43bb1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......k=0.05 smooting performance.......\n",
            "..train..\n",
            "unigram perplexity= 1941.0990998568923\n",
            "bigram perplexity= 95.55443878775249\n",
            "trigram perplexity= 5.3284420937647194\n",
            "..validation..\n",
            "unigram perplexity= 1726.4822060155464\n",
            "bigram perplexity= 315.98910471936813\n",
            "trigram perplexity= 19.734544987407137\n",
            "..test..\n",
            "unigram perplexity= 1685.1386273239207\n",
            "bigram perplexity= 315.73356490457326\n",
            "trigram perplexity= 20.295591321493074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"......k=0.01 smooting performance.......\")\n",
        "\n",
        "print('..train..')\n",
        "print(\"unigram perplexity=\",text_probability(train,1,unigram_train_k_01))\n",
        "print(\"bigram perplexity=\",text_probability(train,2,bigram_train_k_01,unigram_train_k_01))\n",
        "print(\"trigram perplexity=\",text_probability(train,3,trigram_train_k_01,bigram_train_k_01))\n",
        "print('..validation..')\n",
        "print(\"unigram perplexity=\",text_probability(validation,1,unigram_train_k_01))\n",
        "print(\"bigram perplexity=\",text_probability(validation,2,bigram_train_k_01,unigram_train_k_01))\n",
        "print(\"trigram perplexity=\",text_probability(validation,3,trigram_train_k_01,bigram_train_k_01))\n",
        "print('..test..')\n",
        "print(\"unigram perplexity=\",text_probability(test,1,unigram_train_k_01))\n",
        "print(\"bigram perplexity=\",text_probability(test,2,bigram_train_k_01,unigram_train_k_01))\n",
        "print(\"trigram perplexity=\",text_probability(test,3,trigram_train_k_01,bigram_train_k_01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UApapPpY1Aev",
        "outputId": "c81a67b2-0bc2-4b74-bfb6-ec7fb1549328"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......k=0.01 smooting performance.......\n",
            "..train..\n",
            "unigram perplexity= 1941.085645879415\n",
            "bigram perplexity= 96.98183754326558\n",
            "trigram perplexity= 5.409098635628027\n",
            "..validation..\n",
            "unigram perplexity= 1727.3035384219418\n",
            "bigram perplexity= 530.6329428185244\n",
            "trigram perplexity= 39.18001355921677\n",
            "..test..\n",
            "unigram perplexity= 1686.1392615421596\n",
            "bigram perplexity= 527.9992658576441\n",
            "trigram perplexity= 40.471719803450185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "TzHmlyyKK5Wp",
        "outputId": "7ba9401d-437d-4cfe-9c36-fad5487e692f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ce94w5tAnP"
      },
      "source": [
        "## **Problem 2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNWxGLhaByZX"
      },
      "source": [
        "(Eisenstein Ch. 6) Using the Pytorch library, train an LSTM language model from the same Wikitext training corpus you used in problem 1. After each epoch of training, compute its perplexity on the Wikitext validation corpus. Stop training when the perplexity stops improving.\n",
        "\n",
        "1. Fully describe your model architecture, hyperparameters, and experimental procedure.\n",
        "2. After each epoch of training, compute your LM’s perplexity on the development data. Plot the development perplexity against # of epochs. Additionally, compute and report the perplexity on test\n",
        "data.\n",
        "3. Compare experimental results such as perplexity and training time between your n-gram and neural models (include smoothed and unsmooth n-grams). Provide graphs that demonstrate your\n",
        "results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gAYjEiJtvJt"
      },
      "outputs": [],
      "source": [
        "## your code here\n",
        "import numpy as np\n",
        "\n",
        "#build vocabulary\n",
        "\n",
        "train_list = train.split()\n",
        "test_list = test.split()\n",
        "validation_list = validation.split()\n",
        "\n",
        "vocabulary = set(train_list + test_list + validation_list)\n",
        "\n",
        "print(len(vocabulary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ql2HvwNxtf"
      },
      "outputs": [],
      "source": [
        "#vectorize the word\n",
        "\n",
        "word2idx = {w:i for i, w in enumerate(vocabulary)}\n",
        "\n",
        "#test\n",
        "for i,word in enumerate(word2idx):\n",
        "    print('  {:4s}: {:4d},'.format(word, word2idx[word]))\n",
        "    if i == 5:\n",
        "      break\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YeGuh-BPcAT"
      },
      "outputs": [],
      "source": [
        "#vectorize the text\n",
        "def vectorize_text(text):\n",
        "  return [word2idx[word] for word in text]\n",
        "\n",
        "\n",
        "vectorized_train_list = vectorize_text(train_list)\n",
        "vectorized_test_list = vectorize_text(test_list)\n",
        "vectorized_validation_list = vectorize_text(validation_list)\n",
        "print(train_list[0:10])\n",
        "print(vectorized_train_list[0:10])\n",
        "print(word2idx['Valkyria'],word2idx['戦場のヴァルキュリア3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7pl7ImuRocV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "#creating training samples\n",
        "def create_training_samples(sequence_length, vectorized_text,batch_length):\n",
        "  starting_index = np.random.choice(len(vectorized_text)-1-sequence_length, batch_length)\n",
        "  x = np.array([vectorized_text[i:i+sequence_length] for i in starting_index])\n",
        "  y = np.array([vectorized_text[i+1:i+sequence_length+1] for i in starting_index])\n",
        "  return torch.from_numpy(x),torch.from_numpy(y)\n",
        "\n",
        "\n",
        "#test create_training_samples#\n",
        "toy_x, toy_y = create_training_samples(5,vectorized_train_list[0:10],2)\n",
        "\n",
        "print(toy_x)\n",
        "print(toy_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_tZ0Q7g0NJ-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Wikitext:\n",
        "  def __init__(self,vectorized_text,sequence_length):\n",
        "    self.vectorized_text = vectorized_text\n",
        "    self.sequence_length = sequence_length\n",
        "  def __len__(self):\n",
        "    return len(self.vectorized_text) - self.sequence_length\n",
        "  def __getitem__(self,idx):\n",
        "    x = np.array(self.vectorized_text[idx:idx+self.sequence_length])\n",
        "    y = np.array(self.vectorized_text[idx+1:idx+1+self.sequence_length])\n",
        "    return (torch.from_numpy(x),torch.from_numpy(y))\n",
        "    \n",
        "\n",
        "\n",
        "''' test the Wikitesxt dataset \n",
        "wiki_datatest = Wikitext(vectorized_train_list[0:10],5)\n",
        "toy_train_loader = DataLoader(dataset=wiki_datatest,batch_size=100)\n",
        "\n",
        "toy_iter = iter(toy_train_loader)\n",
        "print(vectorized_train_list[0:10])\n",
        "xy = toy_iter.next()\n",
        "x, y = xy\n",
        "print(x, y)\n",
        "'''\n",
        "sequence_length = 10 #64\n",
        "batch_length = 50 #16\n",
        "\n",
        "wiki_datatest_train = Wikitext(vectorized_train_list,sequence_length)\n",
        "wiki_datatest_validation = Wikitext(vectorized_validation_list,sequence_length)\n",
        "wiki_datatest_test = Wikitext(vectorized_test_list,sequence_length)\n",
        "train_loader = DataLoader(dataset=wiki_datatest_train, batch_size=batch_length)\n",
        "validation_loader = DataLoader(dataset=wiki_datatest_validation, batch_size=batch_length)\n",
        "test_loader = DataLoader(dataset=wiki_datatest_test, batch_size=batch_length)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hZHrmsW1LD-"
      },
      "outputs": [],
      "source": [
        "########################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, rnn_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.rnn_layers= rnn_layers\n",
        "        self.hidden_size=hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,rnn_layers,batch_first=True)\n",
        "        self.last_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, text,previous_state):\n",
        "      embeds = self.word_embeddings(text)\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "      lstm_out,new_state = self.lstm(embeds,previous_state)\n",
        "\n",
        "      \n",
        "\n",
        "      lstm_out = lstm_out.reshape(lstm_out.size(0)*lstm_out.size(1), lstm_out.size(2)) #######\n",
        "      \n",
        "\n",
        "      last_layer = self.last_layer(lstm_out)   ######################\n",
        "\n",
        "      return last_layer, new_state\n",
        "    def setup_first_layer(self,sequence_length):\n",
        "      return (torch.zeros(self.rnn_layers, sequence_length, self.hidden_size),\n",
        "                torch.zeros(self.rnn_layers, sequence_length, self.hidden_size))\n",
        "\n",
        "\n",
        "embedding_dim, hidden_dim, vocab_size, rnn_layers = 512, 512, len(vocabulary), 1\n",
        "\n",
        "lstm_model = LSTMModel(embedding_dim, hidden_dim, vocab_size,rnn_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(lstm_model.parameters(), lr=0.01)\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKmSqQoRm-lQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "x, y = create_training_samples(sequence_length,vectorized_train_list, batch_length)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "prev_state = lstm_model.setup_first_layer(batch_length)\n",
        "print(prev_state[0].shape)\n",
        "\n",
        "pred,_ = lstm_model(x,prev_state)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "print(len(prev_state))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XENeeN66bX3a"
      },
      "outputs": [],
      "source": [
        "yshaped = y.reshape(-1)\n",
        "loss = criterion(pred,yshaped)\n",
        "\n",
        "print(yshaped.shape)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGvNcZZGDn5O"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "toy_wiki_datatest = Wikitext(vectorized_train_list[0:10],sequence_length)\n",
        "toy_train_loader = DataLoader(dataset=toy_wiki_datatest,batch_size=batch_length)\n",
        "\n",
        "\n",
        "num_of_epochs = 2\n",
        "\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  lstm_model.train()\n",
        "  current_epoch_train_loss = 0\n",
        "  batch_number = 1\n",
        "  for x,y in train_loader:\n",
        "      #print(x,y)\n",
        "      prev_states = lstm_model.setup_first_layer(batch_length)\n",
        "      optimizer.zero_grad()\n",
        "      pred,new_state = lstm_model(x,prev_states)\n",
        "\n",
        "      new_state[0].detach() ###############################\n",
        "      new_state[1].detach() ###############################\n",
        "\n",
        "      yshaped = y.reshape(-1)\n",
        "      loss = criterion(pred,yshaped)\n",
        "      current_epoch_train_loss+=loss.item()\n",
        "      loss.backward()\n",
        "      clip_grad_norm_(lstm_model.parameters(), 0.5)\n",
        "      optimizer.step()\n",
        "      print('epoch=',epoch,' loss=',loss.item(),' batch_number=',batch_number,\" total=\",len(train_loader))\n",
        "      batch_number+=1\n",
        "\n",
        "\n",
        "  print(\"train loss=\",current_epoch_train_loss/ len(train_loader))\n",
        "\n",
        "\n",
        "  lstm_model.eval()\n",
        "  current_epoch_valid_loss = 0\n",
        "  for x,y in validation_loader:\n",
        "    pred,new_state = lstm_model(x,prev_states)\n",
        "    yshaped = y.reshape(-1)\n",
        "    loss = criterion(pred,yshaped)\n",
        "    current_epoch_valid_loss+=loss.item()\n",
        "    print(\"validation loss=\",loss.item())\n",
        "  \n",
        "  ###########################################################\n",
        "  print(f'Epoch {epoch+1} \\t\\t Training Loss: {current_epoch_train_loss / len(train_loader)} \\t\\t Validation Loss: {current_epoch_valid_loss / len(validation_loader)}')\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgEUWgA2aRKU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#train the model\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(x,y):\n",
        "  prev_states = lstm_model.setup_first_layer(batch_length)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  pred,new_state = lstm_model(x,prev_states)\n",
        "  new_state[0].detach() ###############################\n",
        "  new_state[1].detach() ###############################\n",
        "  yshaped = y.reshape(-1)\n",
        "  loss = criterion(pred,yshaped)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss\n",
        "\n",
        "\n",
        "loss_record = []\n",
        "\n",
        "num_training_iterations=3000\n",
        "for i in tqdm(range(num_training_iterations)):\n",
        "   x, y = create_training_samples(sequence_length,vectorized_train_list, batch_length)\n",
        "   loss = train(x, y)\n",
        "   loss_record.append(loss)\n",
        "   if i%500 == 0:\n",
        "     print(\"i=\",i,' loss=',loss.item(),\" perplexity=\",pow(2,loss.item()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c9a3bd90f3a4324b33788b849f31ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94eeaf7ba1694ad7a84ff91b2b73653c",
              "IPY_MODEL_4214c0c350f24fa98c92058de1c65615",
              "IPY_MODEL_ac08384de96e4ca0b90625f3249c8ed4"
            ],
            "layout": "IPY_MODEL_898131a3885945ed8a7c8de077ce51d8"
          }
        },
        "94eeaf7ba1694ad7a84ff91b2b73653c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d074659f8f3479e96d0bb902e676939",
            "placeholder": "​",
            "style": "IPY_MODEL_58a7ad8f14a949b3945265c252a88ffb",
            "value": "100%"
          }
        },
        "4214c0c350f24fa98c92058de1c65615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f40c0205c4904b0ca8b10188789bbe67",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc1025da4dad4ce28ca67c0ec65a33c9",
            "value": 3
          }
        },
        "ac08384de96e4ca0b90625f3249c8ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded8295a8d7a479a946fdddf55d41de3",
            "placeholder": "​",
            "style": "IPY_MODEL_381f51309e5b47b58acc61fe81aab4a7",
            "value": " 3/3 [00:00&lt;00:00,  8.83it/s]"
          }
        },
        "898131a3885945ed8a7c8de077ce51d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d074659f8f3479e96d0bb902e676939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a7ad8f14a949b3945265c252a88ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f40c0205c4904b0ca8b10188789bbe67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1025da4dad4ce28ca67c0ec65a33c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ded8295a8d7a479a946fdddf55d41de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381f51309e5b47b58acc61fe81aab4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}